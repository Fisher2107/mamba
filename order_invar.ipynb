{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test our best mamba model and transformer solver on 10k test set\n",
    "import torch\n",
    "from model import seq2seq_generate_tour,MambaFull, compute_tour_length,generate_data\n",
    "from benchmarks.transformer_model import TSP_net\n",
    "from tqdm import tqdm\n",
    "#from benchmarks.benchmark_solvers import greedy_tsp\n",
    "\n",
    "coord_dim = 2\n",
    "city_count = [50,100]\n",
    "test_size=10000\n",
    "device = 'cuda'\n",
    "\n",
    "class DotDict(dict):\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self\n",
    "\n",
    "\n",
    "checkpoints = [torch.load('mamba/checkpoints/big/share_50_big_fix_2.pt'),\n",
    "               torch.load('mamba/checkpoints/big/BIG_city100_2.pt')]\n",
    "\n",
    "\n",
    "for i, checkpoint_m in enumerate(checkpoints):\n",
    "    if i>0:\n",
    "        break\n",
    "    args = checkpoint_m['args']\n",
    "    model = MambaFull(args.d_model, args.city_count, args.nb_layers, args.coord_dim, args.mlp_cls,args.B, args.reverse,args.reverse_start,args.mamba2,args.last_layer).to(device)\n",
    "    model.load_state_dict(checkpoint_m['model_baseline_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = generate_data(device, 10, city_count, coord_dim=2 , start = 2)\n",
    "\n",
    "\n",
    "    Loss_mamba = []\n",
    "    with torch.no_grad():\n",
    "        for j in tqdm(range(500)):\n",
    "            tours ,Logprobofactions = seq2seq_generate_tour(device,model,inputs,deterministic=True,lastlayer=args.last_layer,sum_logactions=False)\n",
    "            Loss_mamba.append(compute_tour_length(inputs,tours))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
