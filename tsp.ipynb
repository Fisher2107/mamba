{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1717069023236,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"lbbG1RspOoXC","outputId":"0fba4865-c6c2-4620-dfdb-340124a1d0d3"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.distributions.categorical import Categorical\n","from torch.optim import Adam\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from model import MambaFull, generate_data\n","\n","# Define model parameters and hyperparameters\n","class DotDict(dict):\n","    def __init__(self, **kwds):\n","        self.update(kwds)\n","        self.__dict__ = self\n","\n","args=DotDict()\n","args.bsz=100\n","args.d_model = 64\n","args.coord_dim = 2\n","args.nb_layers = 2\n","args.mlp_cls = nn.Identity #nn.Linear\n","args.city_count = 50\n","args.sequence_length = args.city_count + 1\n","args.deterministic = False #used for sampling from the model"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["seed = 402\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","\n","model = MambaFull(args.d_model, args.city_count, args.nb_layers, args.coord_dim, args.mlp_cls)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":8308710,"status":"ok","timestamp":1717077410267,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"w5jGHXaBOoXM","outputId":"11bd84fd-f86b-412e-b546-f158d22564f6"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["50\n","tensor([ 4, 46,  5, 44, 14, 10, 24, 21, 16, 19, 37, 49, 48, 43,  1,  8,  3, 15,\n","        47, 36, 26, 30, 40,  6, 34, 32, 41, 31, 13, 35, 27, 45,  2, 23, 17,  7,\n","        38,  9, 12,  0, 33, 25, 28, 22, 20, 11, 29, 42, 18, 39],\n","       device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["\"plt.plot(loss_values)\\nplt.xlabel('Epoch')\\nplt.ylabel('Loss')\\nplt.show()\""]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Define a loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Define an optimizer\n","optimizer = Adam(model.parameters(), lr=1e-4)\n","\n","# List to store loss values\n","loss_values = []\n","best_loss = float('inf')\n","\n","# Training loop\n","for epoch in tqdm(range(1)):\n","    # Mask is used to prevent the model from choosing the same city twice\n","    mask = torch.ones(args.bsz, args.city_count).to(device)\n","    #Inputs will have size (bsz, seq_len, coord_dim)\n","    inputs = generate_data(device, args.bsz, args.city_count, args.coord_dim)\n","    # list that will contain Long tensors of shape (bsz,) that gives the idx of the cities chosen at time t\n","    tours = []\n","    # list that will contain Float tensors of shape (bsz,) that gives the log probs of the choices made at time t\n","    sumLogProbOfActions = []\n","    #Construct tour recursively\n","    for i in range(args.city_count):\n","        #print(i)\n","        outputs = model(inputs)[:,-1,:]\n","        #print(outputs[0])\n","        outputs = outputs.masked_fill_(mask == 0, -float('inf'))\n","        #print(outputs[0])\n","        outputs = nn.Softmax(dim=1)(outputs)\n","        #print(outputs[0])\n","        #if args.deterministic:\n","        next_city = torch.argmax(outputs, dim=1)\n","        #print(next_city.shape)\n","        #else:\n","        #    next_city = Categorical(outputs).sample()\n","        #print(next_city[0])\n","        tours.append(next_city)\n","        sumLogProbOfActions.append(torch.log(outputs[torch.arange(args.bsz), next_city]) )\n","        mask[torch.arange(args.bsz), next_city] = 0\n","        inputs = torch.cat((inputs, inputs[torch.arange(args.bsz), next_city, :].unsqueeze(1)), dim=1)\n","        args.sequence_length += 1\n","    print(len(sumLogProbOfActions))\n","    tours = torch.stack(tours, dim=1)\n","    sumLogProbOfActions = torch.stack(sumLogProbOfActions, dim=1).sum(dim=1)\n","\n","    \n","    '''loss = loss_fn(outputs.reshape(-1,7), targets.reshape(-1))\n","\n","    # Backward pass: compute the gradients of the loss with respect to the model's parameters\n","    loss.backward()\n","\n","    # Update the model's parameters\n","    optimizer.step()\n","\n","    # Zero the gradients\n","    optimizer.zero_grad()\n","\n","    # Save checkpoint every 10,000 epochs\n","    if loss.item() < best_loss:\n","        best_loss = loss.item()\n","        checkpoint = {\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss.item()\n","        }\n","        torch.save(checkpoint, 'best_checkpoint.pt')'''\n","    \n","# Plot loss values\n","'''plt.plot(loss_values)\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()'''"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for MambaFull:\n\tMissing key(s) in state_dict: \"norm_f.weight\", \"norm_f.bias\", \"embedding.bias\", \"layers.0.norm.bias\", \"layers.1.norm.bias\". \n\tUnexpected key(s) in state_dict: \"final_norm.weight\". \n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([7, 64]) from checkpoint, the shape in current model is torch.Size([64, 2]).\n\tsize mismatch for layers.0.mixer.A_log: copying a param with shape torch.Size([128, 16]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for layers.0.mixer.x_proj.weight: copying a param with shape torch.Size([36, 128]) from checkpoint, the shape in current model is torch.Size([132, 128]).\n\tsize mismatch for layers.1.mixer.A_log: copying a param with shape torch.Size([128, 16]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for layers.1.mixer.x_proj.weight: copying a param with shape torch.Size([36, 128]) from checkpoint, the shape in current model is torch.Size([132, 128]).\n\tsize mismatch for output_head.weight: copying a param with shape torch.Size([7, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmamba/best_checkpoint.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MambaFull:\n\tMissing key(s) in state_dict: \"norm_f.weight\", \"norm_f.bias\", \"embedding.bias\", \"layers.0.norm.bias\", \"layers.1.norm.bias\". \n\tUnexpected key(s) in state_dict: \"final_norm.weight\". \n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([7, 64]) from checkpoint, the shape in current model is torch.Size([64, 2]).\n\tsize mismatch for layers.0.mixer.A_log: copying a param with shape torch.Size([128, 16]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for layers.0.mixer.x_proj.weight: copying a param with shape torch.Size([36, 128]) from checkpoint, the shape in current model is torch.Size([132, 128]).\n\tsize mismatch for layers.1.mixer.A_log: copying a param with shape torch.Size([128, 16]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for layers.1.mixer.x_proj.weight: copying a param with shape torch.Size([36, 128]) from checkpoint, the shape in current model is torch.Size([132, 128]).\n\tsize mismatch for output_head.weight: copying a param with shape torch.Size([7, 64]) from checkpoint, the shape in current model is torch.Size([50, 64])."]}],"source":["checkpoint = torch.load('mamba/best_checkpoint.pt')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9xbxPNIOoXN"},"outputs":[{"name":"stdout","output_type":"stream","text":["66368\n","tensor([0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 3, 0, 5, 0, 0, 0, 1, 6, 6, 6, 6,\n","        6], device='cuda:0')\n","tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 3, 5,\n","        1], device='cuda:0')\n","accuracy: tensor(1.0000, device='cuda:0')\n","tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 3, 5,\n","        1], device='cuda:0')\n"]}],"source":["print(sum(p.numel() for p in model.parameters()))\n","inputs, targets = generate_data(batch_size=10000)\n","inputs = torch.from_numpy(inputs).long().to(device)\n","targets = torch.from_numpy(targets).long().to(device)\n","outputs = model(inputs)\n","outputs = nn.Softmax(dim=2)(outputs)\n","print(inputs[0])\n","print(outputs[0].argmax(dim=1))\n","print('accuracy:', (outputs.argmax(dim=2) == targets).float().mean())\n","print(targets[0])"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/Fisher2107/mamba-minimal/blob/master/selective_copy.ipynb","timestamp":1717088414324}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
