{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1717069023236,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"lbbG1RspOoXC","outputId":"0fba4865-c6c2-4620-dfdb-340124a1d0d3"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.distributions.categorical import Categorical\n","from torch.optim import Adam\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import time\n","from model import MambaFull, generate_data, seq2seq_generate_tour, compute_tour_length\n","\n","# Define model parameters and hyperparameters\n","class DotDict(dict):\n","    def __init__(self, **kwds):\n","        self.update(kwds)\n","        self.__dict__ = self\n","\n","args=DotDict()\n","#Args for the model\n","\n","args.bsz=100\n","args.d_model = 64\n","args.coord_dim = 2\n","args.nb_layers = 2\n","args.mlp_cls = nn.Identity #nn.Linear\n","args.city_count = 50\n","args.sequence_length = args.city_count + 1\n","args.deterministic = False #used for sampling from the model\n","\n","#Args for the training\n","args.nb_epochs=100\n","args.nb_batch_per_epoch=100 \n","args.nb_batch_eval=10\n","#0 => data will not be recycled and each step new data is generated, however this will make the gpu spend most of the time loading data. Recommeded val is 100\n","args.recycle_data=0 "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#Load checkpoint if available\n","##\n","##To add\n","##\n","tot_time_ckpt = 0\n","\n","#model which will be trained on\n","model_train = MambaFull(args.d_model, args.city_count, args.nb_layers, args.coord_dim, args.mlp_cls)\n","#model which will be used as our baseline in the REINFORCE algorithm. This model will not be trained and only used for evaluation\n","model_baseline = MambaFull(args.d_model, args.city_count, args.nb_layers, args.coord_dim, args.mlp_cls)\n","model_baseline.load_state_dict(model_train.state_dict())\n","model_baseline.eval()\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model_train = model_train.to(device)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":8308710,"status":"ok","timestamp":1717077410267,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"w5jGHXaBOoXM","outputId":"11bd84fd-f86b-412e-b546-f158d22564f6"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["50\n","tensor([ 4, 46,  5, 44, 14, 10, 24, 21, 16, 19, 37, 49, 48, 43,  1,  8,  3, 15,\n","        47, 36, 26, 30, 40,  6, 34, 32, 41, 31, 13, 35, 27, 45,  2, 23, 17,  7,\n","        38,  9, 12,  0, 33, 25, 28, 22, 20, 11, 29, 42, 18, 39],\n","       device='cuda:0')\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["\"plt.plot(loss_values)\\nplt.xlabel('Epoch')\\nplt.ylabel('Loss')\\nplt.show()\""]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Define a loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Define an optimizer\n","optimizer = Adam(model_train.parameters(), lr=1e-4)\n","\n","\n","mean_tour_length_train_list = [] # List to store loss values\n","mean_tour_length_train_best = float('inf') # Variable to store the best loss value\n","best_loss = float('inf')\n","\n","start_training_time = time.time()\n","\n","# Training loop\n","for epoch in tqdm(range(args.nb_epochs)):\n","    model_train.train()\n","    i= 0 # Tracks the number of steps before we generate new data\n","    start = time.time()\n","    for step in range(args.nb_batch_per_epoch):\n","\n","        if i == 0:\n","            #Inputs will have size (bsz, seq_len, coord_dim)\n","            inputs = generate_data(device, args.bsz, args.city_count, args.coord_dim)\n","            i=args.recycle_data\n","        else: i-=1\n","\n","        # list that will contain Long tensors of shape (bsz,) that gives the idx of the cities chosen at time t\n","        tours_train, sumLogProbOfActions = seq2seq_generate_tour(args,device,model_train,inputs)\n","        tours_baseline, _ = seq2seq_generate_tour(args,device,model_baseline,inputs)\n","\n","        #get the length of the tours\n","        L_train = compute_tour_length(inputs, tours_train)\n","        if i==args.recycle_data: #if we are using new data, we need to compute the baseline tour length\n","            L_baseline = compute_tour_length(inputs, tours_baseline)\n","\n","        # backprop\n","        loss = torch.mean( (L_train - L_baseline)* sumLogProbOfActions )\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    time_one_epoch = time.time()-start\n","    time_tot = time.time()-start_training_time + tot_time_ckpt\n","\n","    ###################\n","    # Evaluate train model and baseline on 10k random TSP instances\n","    ###################\n","    model_train.eval()\n","    mean_tour_length_train = 0\n","    mean_tour_length_baseline = 0\n","    for step in range(0,args.nb_batch_eval):\n","\n","        # generate a batch of random tsp instances   \n","        x = torch.rand(args.bsz, args.nb_nodes, args.dim_input_nodes, device=device) \n","\n","        # compute tour for model and baseline\n","        with torch.no_grad():\n","            tour_train, _ = model_train(x, deterministic=True)\n","            tour_baseline, _ = model_baseline(x, deterministic=True)\n","            \n","        # get the lengths of the tours\n","        L_train = compute_tour_length(x, tour_train)\n","        L_baseline = compute_tour_length(x, tour_baseline)\n","\n","        # L_tr and L_bl are tensors of shape (bsz,). Compute the mean tour length\n","        mean_tour_length_train += L_train.mean().item()\n","        mean_tour_length_baseline += L_baseline.mean().item()\n","\n","    mean_tour_length_train =  mean_tour_length_train/ args.nb_batch_eval\n","    mean_tour_length_baseline =  mean_tour_length_baseline/ args.nb_batch_eval\n","    print(f'Epoch {epoch}, mean tour length train: {mean_tour_length_train}, mean tour length baseline: {mean_tour_length_baseline}, time one epoch: {time_one_epoch}, time tot: {time_tot}')\n","\n","    mean_tour_length_train_list.append(mean_tour_length_train)\n","    # evaluate train model and baseline and update if train model is better\n","    if mean_tour_length_train < mean_tour_length_baseline:\n","        model_baseline.load_state_dict( model_train.state_dict() )\n","\n","    # Save checkpoint every 10,000 epochs\n","    if mean_tour_length_train > mean_tour_length_train_best:\n","        mean_tour_length_train_best = mean_tour_length_train\n","        checkpoint = {\n","            'epoch': epoch,\n","            'model_state_dict': model_train.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'mean_tour_length_list': mean_tour_length_train_list,\n","        }\n","        torch.save(checkpoint, 'best_checkpoint.pt')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for MambaFull:\n\tMissing key(s) in state_dict: \"norm_f.weight\", \"norm_f.bias\", \"embedding.bias\", \"layers.0.norm.bias\", \"layers.1.norm.bias\". \n\tUnexpected key(s) in state_dict: \"final_norm.weight\". \n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([7, 64]) from checkpoint, the shape in current model is torch.Size([64, 2]).\n\tsize mismatch for layers.0.mixer.A_log: copying a param with shape torch.Size([128, 16]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for layers.0.mixer.x_proj.weight: copying a param with shape torch.Size([36, 128]) from checkpoint, the shape in current model is torch.Size([132, 128]).\n\tsize mismatch for layers.1.mixer.A_log: copying a param with shape torch.Size([128, 16]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for layers.1.mixer.x_proj.weight: copying a param with shape torch.Size([36, 128]) from checkpoint, the shape in current model is torch.Size([132, 128]).\n\tsize mismatch for output_head.weight: copying a param with shape torch.Size([7, 64]) from checkpoint, the shape in current model is torch.Size([50, 64]).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmamba/best_checkpoint.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MambaFull:\n\tMissing key(s) in state_dict: \"norm_f.weight\", \"norm_f.bias\", \"embedding.bias\", \"layers.0.norm.bias\", \"layers.1.norm.bias\". \n\tUnexpected key(s) in state_dict: \"final_norm.weight\". \n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([7, 64]) from checkpoint, the shape in current model is torch.Size([64, 2]).\n\tsize mismatch for layers.0.mixer.A_log: copying a param with shape torch.Size([128, 16]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for layers.0.mixer.x_proj.weight: copying a param with shape torch.Size([36, 128]) from checkpoint, the shape in current model is torch.Size([132, 128]).\n\tsize mismatch for layers.1.mixer.A_log: copying a param with shape torch.Size([128, 16]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for layers.1.mixer.x_proj.weight: copying a param with shape torch.Size([36, 128]) from checkpoint, the shape in current model is torch.Size([132, 128]).\n\tsize mismatch for output_head.weight: copying a param with shape torch.Size([7, 64]) from checkpoint, the shape in current model is torch.Size([50, 64])."]}],"source":["checkpoint = torch.load('mamba/best_checkpoint.pt')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9xbxPNIOoXN"},"outputs":[{"name":"stdout","output_type":"stream","text":["66368\n","tensor([0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 3, 0, 5, 0, 0, 0, 1, 6, 6, 6, 6,\n","        6], device='cuda:0')\n","tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 3, 5,\n","        1], device='cuda:0')\n","accuracy: tensor(1.0000, device='cuda:0')\n","tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 3, 5,\n","        1], device='cuda:0')\n"]}],"source":["print(sum(p.numel() for p in model.parameters()))\n","inputs, targets = generate_data(batch_size=10000)\n","inputs = torch.from_numpy(inputs).long().to(device)\n","targets = torch.from_numpy(targets).long().to(device)\n","outputs = model(inputs)\n","outputs = nn.Softmax(dim=2)(outputs)\n","print(inputs[0])\n","print(outputs[0].argmax(dim=1))\n","print('accuracy:', (outputs.argmax(dim=2) == targets).float().mean())\n","print(targets[0])"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/Fisher2107/mamba-minimal/blob/master/selective_copy.ipynb","timestamp":1717088414324}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
